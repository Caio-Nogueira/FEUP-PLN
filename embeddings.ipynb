{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b6b3f8",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2vec produces one vector per word, whereas BoW produces one number (a wordcount). Word2vec is great for digging into documents and identifying content and subsets of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2017f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_excel(\"data/OpArticles.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f0328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "documents = [re.sub(\"[^a-zA-Z]\", \" \", body.lower()) for body in articles[\"body\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b31ec1",
   "metadata": {},
   "source": [
    "## Pre-trained embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d8b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_pt = KeyedVectors.load_word2vec_format('./word_vectors/skip_s100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d6c7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cachorro', 0.8545767068862915),\n",
       " ('gato', 0.8402905464172363),\n",
       " ('monstro', 0.8336718082427979),\n",
       " ('pássaro', 0.8299859166145325),\n",
       " ('ogro', 0.8261020183563232),\n",
       " ('gorila', 0.819529116153717),\n",
       " ('furão', 0.813050389289856),\n",
       " ('cãozinho', 0.8127977848052979),\n",
       " ('felino', 0.8087738752365112),\n",
       " ('filhote', 0.8069430589675903)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pt.most_similar(positive=[\"cão\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79c0fbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rainha-consorte', 0.7912216186523438),\n",
       " ('primogénita', 0.7738461494445801),\n",
       " ('imperatriz-mãe', 0.7646884322166443),\n",
       " ('paleóloga', 0.752788245677948),\n",
       " ('dama-de-companhia', 0.7478024363517761),\n",
       " ('consorte', 0.7475903630256653),\n",
       " ('princesa-eleitora', 0.7472771406173706),\n",
       " ('piroska', 0.7468665838241577),\n",
       " ('ulrica', 0.7454056143760681),\n",
       " ('ranavalona', 0.7441917657852173)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pt.most_similar(positive=[\"rei\", \"mulher\"], negative=[\"homem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e96a7",
   "metadata": {},
   "source": [
    "### Loading text spans dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd1bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('data/OpArticles_ADUs.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755f98d",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "227354c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pt_core_news_sm\n",
    "nlp = pt_core_news_sm.load()\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, dataset['tokens'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['tokens'][i]).lower()\n",
    "    review = ' '.join([word.lemma_.lower().strip() + word.pos_ for word in nlp(review)])\n",
    "    # add review to corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45acb9e",
   "metadata": {},
   "source": [
    "## Fixing the length of the input\n",
    "The reviews in our corpus have variable length. However, we need to represent them with a fixed-length vector of features. One way to do it is to impose a limit on the number of word embeddings we want to include.\n",
    "\n",
    "To convert words into their vector representations (embeddings), let's create an auxiliary function that takes in the number of embeddings we wish to include in the representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece3119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    \n",
    "    # split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "        finally:\n",
    "            i += 1\n",
    "    \n",
    "    # add blanks up to sequence_len, if needed\n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3978bea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 92 16.132174640148122 10.750637692983176 ModeResult(mode=array([10]), count=array([916]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b3098",
   "metadata": {},
   "source": [
    "The average length of the text spans is 16.1 tokens with standard deviation of 10.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee51dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16743, 2000) (16743,)\n"
     ]
    }
   ],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "embeddings_corpus = []\n",
    "for c in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(model_pt, c, 20))\n",
    "\n",
    "X = np.array(embeddings_corpus)\n",
    "y = dataset['label']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86a7d815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "\n",
      "Confusion matrix:\n",
      " [[   0    0  733    0    0]\n",
      " [   0    0  133    0    0]\n",
      " [   0    0 1621    0    0]\n",
      " [   0    0  282    0    0]\n",
      " [   0    0  580    0    0]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.00      0.00      0.00       733\n",
      "      Policy       0.00      0.00      0.00       133\n",
      "       Value       0.48      1.00      0.65      1621\n",
      "    Value(+)       0.00      0.00      0.00       282\n",
      "    Value(-)       0.00      0.00      0.00       580\n",
      "\n",
      "    accuracy                           0.48      3349\n",
      "   macro avg       0.10      0.20      0.13      3349\n",
      "weighted avg       0.23      0.48      0.32      3349\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caion\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\caion\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\caion\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, stratify=y)\n",
    "max_iterations = 10**6/np.ceil(X_train.shape[0] / 10) #Empirically,the model converges after 10^6/n where n \n",
    "                                        # the size of the test set\n",
    "\n",
    "sgd_model = SGDClassifier(penalty=\"l2\", \n",
    "                          max_iter=np.ceil(max_iterations/10), average=True)\n",
    "sgd_gs = GridSearchCV(sgd_model, n_jobs=-1, param_grid={}, cv=10, scoring=\"accuracy\", verbose = 1)\n",
    "sgd_gs.fit(X_train, y_train)\n",
    "y_pred = sgd_gs.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300db74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
