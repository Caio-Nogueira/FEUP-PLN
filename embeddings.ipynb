{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b6b3f8",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2vec produces one vector per word, whereas BoW produces one number (a wordcount). Word2vec is great for digging into documents and identifying content and subsets of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2017f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles = pd.read_excel(\"data/OpArticles.xlsx\")\n",
    "dataset = pd.read_excel('data/OpArticles_ADUs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "documents = [re.sub(\"[^a-zA-Z]\", \" \", body.lower()) for body in articles[\"body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffb37b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 7.778205871582031\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "start_time = time()\n",
    "model_articles = Word2Vec(documents, vector_size=150, window=10, min_count=2, workers=10, sg=1)\n",
    "\n",
    "print(\"Training time:\", time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49f37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles.wv.save(\"./word_vectors/model_articles\")\n",
    "\n",
    "model_articles = KeyedVectors.load(\"./word_vectors/model_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b31ec1",
   "metadata": {},
   "source": [
    "## Pre-trained (NILC) embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84d8b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Word2Vec SKIP-GRAM 100 dimensions\n",
    "model_skip_s100 = KeyedVectors.load_word2vec_format('./word_vectors/skip_s100.txt')\n",
    "\n",
    "#FastText SKIP-GRAM 100 dimensions\n",
    "ft_skip_s100 = KeyedVectors.load_word2vec_format('./word_vectors/ft_skip_s100.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755f98d",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "227354c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pt_core_news_sm\n",
    "#nlp = pt_core_news_sm.load()\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, dataset['tokens'].size):\n",
    "    # get review, remove non alpha chars and convert to lower-case\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['tokens'][i]).lower()\n",
    "    #review = ' '.join([word.lemma_.lower().strip() + word.pos_ for word in nlp(review)])\n",
    "    # add review to corpus\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45acb9e",
   "metadata": {},
   "source": [
    "## Fixing the length of the input\n",
    "The reviews in our corpus have variable length. However, we need to represent them with a fixed-length vector of features. One way to do it is to impose a limit on the number of word embeddings we want to include.\n",
    "\n",
    "To convert words into their vector representations (embeddings), let's create an auxiliary function that takes in the number of embeddings we wish to include in the representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece3119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    \n",
    "    # split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            True   # simply ignore out-of-vocabulary tokens\n",
    "        finally:\n",
    "            i += 1\n",
    "    \n",
    "    # add blanks up to sequence_len, if needed\n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3978bea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 92 16.132174640148122 10.750637692983176 ModeResult(mode=array([10]), count=array([916]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b3098",
   "metadata": {},
   "source": [
    "The average length of the text spans is 16.1 tokens with standard deviation of 10.75."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fab132",
   "metadata": {},
   "source": [
    "## Generating training/testing sets\n",
    "\n",
    "In this experimental setup, we will be using different word embedding models. `model_articles` was trained using the full articles dataset whereas the others were loaded and pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee51dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert corpus into dataset with appended embeddings representation\n",
    "\n",
    "def generate_sets(model):\n",
    "    embeddings_corpus = []\n",
    "    for c in corpus:\n",
    "        embeddings_corpus.append(text_to_vector(model, c, 15))\n",
    "\n",
    "    X = np.array(embeddings_corpus)\n",
    "    y = dataset['label']\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c057274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_articles, y_articles = generate_sets(model_skip_s100)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_articles, y_articles, test_size = 0.20, random_state = 0, stratify=y_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a7d815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 226   16  375   32   84]\n",
      " [   8   49   65    4    7]\n",
      " [ 224   47 1074   78  198]\n",
      " [  36    8  156   51   31]\n",
      " [  67   13  328   15  157]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Fact       0.40      0.31      0.35       733\n",
      "      Policy       0.37      0.37      0.37       133\n",
      "       Value       0.54      0.66      0.59      1621\n",
      "    Value(+)       0.28      0.18      0.22       282\n",
      "    Value(-)       0.33      0.27      0.30       580\n",
      "\n",
      "    accuracy                           0.46      3349\n",
      "   macro avg       0.38      0.36      0.37      3349\n",
      "weighted avg       0.44      0.46      0.45      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_gs = GridSearchCV(sgd_model, n_jobs=-1, param_grid={}, cv=10, scoring=\"accuracy\", verbose = 1)\n",
    "sgd_gs.fit(X_train, y_train)\n",
    "y_pred = sgd_gs.predict(X_test)\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification report:\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300db74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
