{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7787428e",
   "metadata": {},
   "source": [
    "# Using HuggingFace to train a model\n",
    "\n",
    "In this notebook, we will be using `distilbert-base-uncased-finetuned-sst-2-english` to train a text classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe098d5",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Firstly, we need to merge the translated dataset with the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab8e367d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>node</th>\n",
       "      <th>ranges</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2516, 2556]]</td>\n",
       "      <td>O facto não é apenas fruto da ignorância</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>[[2568, 2806]]</td>\n",
       "      <td>havia no seu humor mais jornalismo (mais inves...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>[[3169, 3190]]</td>\n",
       "      <td>É tudo cómico na FIFA</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>[[3198, 3285]]</td>\n",
       "      <td>o que todos nós permitimos que esta organizaçã...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>[[4257, 4296]]</td>\n",
       "      <td>não nos fazem rir à custa dos poderosos</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16738</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>29</td>\n",
       "      <td>[[4980, 5041], [5074, 5279]]</td>\n",
       "      <td>A única variável disponibilizada que pode ser ...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16739</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>30</td>\n",
       "      <td>[[5293, 5340]]</td>\n",
       "      <td>esse número esconde informação muito pertinente</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16740</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>32</td>\n",
       "      <td>[[5053, 5072]]</td>\n",
       "      <td>bastante imperfeita</td>\n",
       "      <td>Value(-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16741</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>34</td>\n",
       "      <td>[[5549, 5643]]</td>\n",
       "      <td>esconde também a proporção de diplomados que e...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16742</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>35</td>\n",
       "      <td>[[5488, 5537]]</td>\n",
       "      <td>esconde a distribuição de salários dos diplomados</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     article_id annotator  node                        ranges  \\\n",
       "0      5d04a31b896a7fea069ef06f         A     0                [[2516, 2556]]   \n",
       "1      5d04a31b896a7fea069ef06f         A     1                [[2568, 2806]]   \n",
       "2      5d04a31b896a7fea069ef06f         A     3                [[3169, 3190]]   \n",
       "3      5d04a31b896a7fea069ef06f         A     4                [[3198, 3285]]   \n",
       "4      5d04a31b896a7fea069ef06f         A     6                [[4257, 4296]]   \n",
       "...                         ...       ...   ...                           ...   \n",
       "16738  5cf4b764896a7fea06032673         D    29  [[4980, 5041], [5074, 5279]]   \n",
       "16739  5cf4b764896a7fea06032673         D    30                [[5293, 5340]]   \n",
       "16740  5cf4b764896a7fea06032673         D    32                [[5053, 5072]]   \n",
       "16741  5cf4b764896a7fea06032673         D    34                [[5549, 5643]]   \n",
       "16742  5cf4b764896a7fea06032673         D    35                [[5488, 5537]]   \n",
       "\n",
       "                                                  tokens     label  \n",
       "0               O facto não é apenas fruto da ignorância     Value  \n",
       "1      havia no seu humor mais jornalismo (mais inves...     Value  \n",
       "2                                  É tudo cómico na FIFA     Value  \n",
       "3      o que todos nós permitimos que esta organizaçã...     Value  \n",
       "4                não nos fazem rir à custa dos poderosos     Value  \n",
       "...                                                  ...       ...  \n",
       "16738  A única variável disponibilizada que pode ser ...     Value  \n",
       "16739    esse número esconde informação muito pertinente      Fact  \n",
       "16740                                bastante imperfeita  Value(-)  \n",
       "16741  esconde também a proporção de diplomados que e...     Value  \n",
       "16742  esconde a distribuição de salários dos diplomados      Fact  \n",
       "\n",
       "[16743 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_excel(\"../data/OpArticles_ADUs.xlsx\")\n",
    "translated_dataset = pd.read_csv(\"../data/translated_spans.csv\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15e8fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presumo que essas partilhas tenham gerado um e...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5cdd971b896a7fea062d6e3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>essas partilhas tenham gerado um efeito bola d...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5cdd971b896a7fea062d6e3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esta questão ter [justificadamente] despertado...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5cdd971b896a7fea062d6e3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a ocasião propicia um debate amplo na sociedad...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5cdd971b896a7fea062d6e3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a tomada urgente de medidas por parte da tutel...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5cdd971b896a7fea062d6e3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>Um presidente de câmara pode pertencer à admin...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5d04c671896a7fea06a11275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>eticamente é reprovável</td>\n",
       "      <td>Value(-)</td>\n",
       "      <td>5d04c671896a7fea06a11275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10250</th>\n",
       "      <td>eticamente é reprovável e, o bom senso, aconse...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5d04c671896a7fea06a11275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>o bom senso, aconselha a não o fazer</td>\n",
       "      <td>Value</td>\n",
       "      <td>5d04c671896a7fea06a11275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10252</th>\n",
       "      <td>Um presidente de câmara pode ter a sua preferê...</td>\n",
       "      <td>Value</td>\n",
       "      <td>5d04c671896a7fea06a11275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10253 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens     label  \\\n",
       "0      presumo que essas partilhas tenham gerado um e...     Value   \n",
       "1      essas partilhas tenham gerado um efeito bola d...     Value   \n",
       "2      esta questão ter [justificadamente] despertado...     Value   \n",
       "3      a ocasião propicia um debate amplo na sociedad...     Value   \n",
       "4      a tomada urgente de medidas por parte da tutel...     Value   \n",
       "...                                                  ...       ...   \n",
       "10248  Um presidente de câmara pode pertencer à admin...     Value   \n",
       "10249                            eticamente é reprovável  Value(-)   \n",
       "10250  eticamente é reprovável e, o bom senso, aconse...     Value   \n",
       "10251               o bom senso, aconselha a não o fazer     Value   \n",
       "10252  Um presidente de câmara pode ter a sua preferê...     Value   \n",
       "\n",
       "                     article_id  \n",
       "0      5cdd971b896a7fea062d6e3d  \n",
       "1      5cdd971b896a7fea062d6e3d  \n",
       "2      5cdd971b896a7fea062d6e3d  \n",
       "3      5cdd971b896a7fea062d6e3d  \n",
       "4      5cdd971b896a7fea062d6e3d  \n",
       "...                         ...  \n",
       "10248  5d04c671896a7fea06a11275  \n",
       "10249  5d04c671896a7fea06a11275  \n",
       "10250  5d04c671896a7fea06a11275  \n",
       "10251  5d04c671896a7fea06a11275  \n",
       "10252  5d04c671896a7fea06a11275  \n",
       "\n",
       "[10253 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grouped_df = dataset.groupby(by=['article_id', 'ranges'])\n",
    "dataset_dict = {\"tokens\": [], \"label\": [], \"article_id\": []}\n",
    "\n",
    "for i, group in grouped_df:\n",
    "    dict_counts = {x: group[\"label\"].value_counts()[x] for x in np.unique(group[['label']].values)}\n",
    "    if len(dict_counts.keys()) > 1:\n",
    "        continue\n",
    "    dataset_dict[\"article_id\"].append(group[\"article_id\"].values[0])\n",
    "    dataset_dict[\"tokens\"].append(group[\"tokens\"].values[0])\n",
    "    dataset_dict[\"label\"].append(list(dict_counts.keys())[0])\n",
    "    \n",
    "dataset = pd.DataFrame(dataset_dict, columns = [\"tokens\", \"label\", \"article_id\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7383c01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value       5003\n",
       "Fact        2235\n",
       "Value(-)    1768\n",
       "Value(+)     849\n",
       "Policy       398\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = dataset.join(translated_dataset)\n",
    "dataset[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0fd9f7",
   "metadata": {},
   "source": [
    "### Converting it into a Hugging Face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets --quiet\n",
    "from datasets import Dataset\n",
    "\n",
    "labels = ['Value','Fact','Value(+)','Value(-)','Policy']\n",
    "numeric_labels = []\n",
    "\n",
    "for label in dataset[\"label\"].values:\n",
    "    new_label = labels.index(label)\n",
    "    numeric_labels.append(new_label)\n",
    "\n",
    "dataset[\"label\"] = numeric_labels\n",
    "\n",
    "dataset_hf = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3386cb",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d8182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 90% train, 10% test+validation\n",
    "train_test = dataset_hf.train_test_split(test_size=0.1)\n",
    "\n",
    "# Split the 10% test+validation set in half test, half validation\n",
    "valid_test = train_test['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_valid_test_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb94c69",
   "metadata": {},
   "source": [
    "### Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed39caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1725b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac612ba21a448eeacf14aadbae9a140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb64c52df779410b897c20849a8dc018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47145719abbd4fd786d81517666b3c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e176a8fd97442d7a12500a85c28c7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422e3f9b0a9b46f6898b51cee0c3f0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, padding=True, truncation=True, model_max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aef4f",
   "metadata": {},
   "source": [
    "## Apply the tokenizer loaded into the text spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bef998b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a7737bb0a34c7aa3ef7784c95e082a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745650bbeb67431eb3f6249af8599728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451309ea5a844748b7cd01e96d59a097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'label', 'article_id'],\n",
       "        num_rows: 9227\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'label', 'article_id'],\n",
       "        num_rows: 513\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'label', 'article_id'],\n",
       "        num_rows: 513\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(sample):\n",
    "    return tokenizer(sample[\"tokens\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = train_valid_test_dataset.map(preprocess_function, batched=True)\n",
    "train_valid_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d6cfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1968, 0.1936, 0.2209, 0.1935, 0.1952]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caion\\AppData\\Local\\Temp/ipykernel_15632/361294440.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = torch.nn.functional.softmax(outputs.logits)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(train_valid_test_dataset['test'][0]['tokens'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bbdcb4",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "The next step is to fine-tune the model to our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ce7c775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e2e23efcb74cb996a043e7f87c37d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\"test_trainer\", num_train_epochs=1)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebb1c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\caion\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9227\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 577\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='577' max='577' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [577/577 24:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.295900</td>\n",
       "      <td>1.300379</td>\n",
       "      <td>0.499025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 513\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-577\n",
      "Configuration saved in ./results\\checkpoint-577\\config.json\n",
      "Model weights saved in ./results\\checkpoint-577\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-577\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-577\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results\\checkpoint-577 (score: 1.3003790378570557).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=577, training_loss=1.291823137570915, metrics={'train_runtime': 1494.1696, 'train_samples_per_second': 6.175, 'train_steps_per_second': 0.386, 'total_flos': 369745604368440.0, 'train_loss': 1.291823137570915, 'epoch': 1.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b79270f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 513\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3003790378570557,\n",
       " 'eval_accuracy': 0.49902534113060426,\n",
       " 'eval_runtime': 19.6469,\n",
       " 'eval_samples_per_second': 26.111,\n",
       " 'eval_steps_per_second': 1.68,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a107fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\n",
      "Configuration saved in ./results\\config.json\n",
      "Model weights saved in ./results\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8ea39",
   "metadata": {},
   "source": [
    "### Loading and using a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1758690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./results\\added_tokens.json. We won't load it.\n",
      "loading file ./results\\vocab.txt\n",
      "loading file ./results\\tokenizer.json\n",
      "loading file None\n",
      "loading file ./results\\special_tokens_map.json\n",
      "loading file ./results\\tokenizer_config.json\n",
      "loading configuration file ./results\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./results\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./results\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./results.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"./results\")\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"./results\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f947be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model2, tokenizer=tokenizer2) #, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b03379d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[248   6   0   0   0]\n",
      " [ 84  34   0   0   0]\n",
      " [ 44   2   0   0   0]\n",
      " [ 76   3   0   0   0]\n",
      " [ 16   0   0   0   0]]\n",
      "Accuracy:  0.5497076023391813\n",
      "Precision:  0.2570940170940171\n",
      "Recall:  0.25290270919524893\n",
      "F1:  0.22083170470574237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caion\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred= []\n",
    "for p in tokenized_dataset['test']['tokens']:\n",
    "    ti = tokenizer2(p, return_tensors=\"pt\")\n",
    "    out = model2(**ti)\n",
    "    pred = torch.argmax(out.logits)\n",
    "    y_pred.append(pred)   # our labels are already 0 and 1\n",
    "\n",
    "\n",
    "y_test = tokenized_dataset['test']['label']\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702b207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
