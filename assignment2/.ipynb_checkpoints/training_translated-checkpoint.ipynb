{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7787428e",
   "metadata": {},
   "source": [
    "# Using HuggingFace to train a model\n",
    "\n",
    "In this notebook, we will be using `distilbert-base-uncased-finetuned-sst-2-english` to train a text classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe098d5",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Firstly, we need to merge the translated dataset with the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8e367d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>node</th>\n",
       "      <th>ranges</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>[[2516, 2556]]</td>\n",
       "      <td>O facto não é apenas fruto da ignorância</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>[[2568, 2806]]</td>\n",
       "      <td>havia no seu humor mais jornalismo (mais inves...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>[[3169, 3190]]</td>\n",
       "      <td>É tudo cómico na FIFA</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>[[3198, 3285]]</td>\n",
       "      <td>o que todos nós permitimos que esta organizaçã...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d04a31b896a7fea069ef06f</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>[[4257, 4296]]</td>\n",
       "      <td>não nos fazem rir à custa dos poderosos</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16738</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>29</td>\n",
       "      <td>[[4980, 5041], [5074, 5279]]</td>\n",
       "      <td>A única variável disponibilizada que pode ser ...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16739</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>30</td>\n",
       "      <td>[[5293, 5340]]</td>\n",
       "      <td>esse número esconde informação muito pertinente</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16740</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>32</td>\n",
       "      <td>[[5053, 5072]]</td>\n",
       "      <td>bastante imperfeita</td>\n",
       "      <td>Value(-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16741</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>34</td>\n",
       "      <td>[[5549, 5643]]</td>\n",
       "      <td>esconde também a proporção de diplomados que e...</td>\n",
       "      <td>Value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16742</th>\n",
       "      <td>5cf4b764896a7fea06032673</td>\n",
       "      <td>D</td>\n",
       "      <td>35</td>\n",
       "      <td>[[5488, 5537]]</td>\n",
       "      <td>esconde a distribuição de salários dos diplomados</td>\n",
       "      <td>Fact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16743 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     article_id annotator  node                        ranges  \\\n",
       "0      5d04a31b896a7fea069ef06f         A     0                [[2516, 2556]]   \n",
       "1      5d04a31b896a7fea069ef06f         A     1                [[2568, 2806]]   \n",
       "2      5d04a31b896a7fea069ef06f         A     3                [[3169, 3190]]   \n",
       "3      5d04a31b896a7fea069ef06f         A     4                [[3198, 3285]]   \n",
       "4      5d04a31b896a7fea069ef06f         A     6                [[4257, 4296]]   \n",
       "...                         ...       ...   ...                           ...   \n",
       "16738  5cf4b764896a7fea06032673         D    29  [[4980, 5041], [5074, 5279]]   \n",
       "16739  5cf4b764896a7fea06032673         D    30                [[5293, 5340]]   \n",
       "16740  5cf4b764896a7fea06032673         D    32                [[5053, 5072]]   \n",
       "16741  5cf4b764896a7fea06032673         D    34                [[5549, 5643]]   \n",
       "16742  5cf4b764896a7fea06032673         D    35                [[5488, 5537]]   \n",
       "\n",
       "                                                  tokens     label  \n",
       "0               O facto não é apenas fruto da ignorância     Value  \n",
       "1      havia no seu humor mais jornalismo (mais inves...     Value  \n",
       "2                                  É tudo cómico na FIFA     Value  \n",
       "3      o que todos nós permitimos que esta organizaçã...     Value  \n",
       "4                não nos fazem rir à custa dos poderosos     Value  \n",
       "...                                                  ...       ...  \n",
       "16738  A única variável disponibilizada que pode ser ...     Value  \n",
       "16739    esse número esconde informação muito pertinente      Fact  \n",
       "16740                                bastante imperfeita  Value(-)  \n",
       "16741  esconde também a proporção de diplomados que e...     Value  \n",
       "16742  esconde a distribuição de salários dos diplomados      Fact  \n",
       "\n",
       "[16743 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_excel(\"../data/OpArticles_ADUs.xlsx\")\n",
    "translated_dataset = pd.read_csv(\"../data/translated_spans.csv\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e8fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>presumo que essas partilhas tenham gerado um e...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>essas partilhas tenham gerado um efeito bola d...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esta questão ter [justificadamente] despertado...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a ocasião propicia um debate amplo na sociedad...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a tomada urgente de medidas por parte da tutel...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>Um presidente de câmara pode pertencer à admin...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>eticamente é reprovável</td>\n",
       "      <td>Value(-)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10250</th>\n",
       "      <td>eticamente é reprovável e, o bom senso, aconse...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>o bom senso, aconselha a não o fazer</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10252</th>\n",
       "      <td>Um presidente de câmara pode ter a sua preferê...</td>\n",
       "      <td>Value</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10253 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens     label article_id\n",
       "0      presumo que essas partilhas tenham gerado um e...     Value        NaN\n",
       "1      essas partilhas tenham gerado um efeito bola d...     Value        NaN\n",
       "2      esta questão ter [justificadamente] despertado...     Value        NaN\n",
       "3      a ocasião propicia um debate amplo na sociedad...     Value        NaN\n",
       "4      a tomada urgente de medidas por parte da tutel...     Value        NaN\n",
       "...                                                  ...       ...        ...\n",
       "10248  Um presidente de câmara pode pertencer à admin...     Value        NaN\n",
       "10249                            eticamente é reprovável  Value(-)        NaN\n",
       "10250  eticamente é reprovável e, o bom senso, aconse...     Value        NaN\n",
       "10251               o bom senso, aconselha a não o fazer     Value        NaN\n",
       "10252  Um presidente de câmara pode ter a sua preferê...     Value        NaN\n",
       "\n",
       "[10253 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grouped_df = dataset.groupby(by=['article_id', 'ranges'])\n",
    "dataset_dict = {\"tokens\": [], \"label\": []}\n",
    "\n",
    "for i, group in grouped_df:\n",
    "    dict_counts = {x: group[\"label\"].value_counts()[x] for x in np.unique(group[['label']].values)}\n",
    "    if len(dict_counts.keys()) > 1:\n",
    "        continue\n",
    "    #dataset_dict[\"article_id\"].append(group[\"article_id\"].values[0])\n",
    "    dataset_dict[\"tokens\"].append(group[\"tokens\"].values[0])\n",
    "    dataset_dict[\"label\"].append(list(dict_counts.keys())[0])\n",
    "    \n",
    "dataset = pd.DataFrame(dataset_dict, columns = [\"tokens\", \"label\", \"article_id\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7383c01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value       5003\n",
       "Fact        2235\n",
       "Value(-)    1768\n",
       "Value(+)     849\n",
       "Policy       398\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = dataset.join(translated_dataset)\n",
    "dataset[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0fd9f7",
   "metadata": {},
   "source": [
    "### Converting it into a Hugging Face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeca36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets --quiet\n",
    "from datasets import Dataset\n",
    "\n",
    "labels = ['Value','Fact','Value(+)','Value(-)','Policy']\n",
    "numeric_labels = []\n",
    "\n",
    "for label in dataset[\"label\"].values:\n",
    "    new_label = labels.index(label)\n",
    "    numeric_labels.append(new_label)\n",
    "\n",
    "dataset[\"label\"] = numeric_labels\n",
    "\n",
    "dataset_hf = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee950577",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"tokens\"] = translated_dataset[\"Translated Spans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1875d54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>article_id</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I assume these shares have generated a snowbal...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I assume these shares have generated a snowbal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These shares have generated a snowball effect</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These shares have generated a snowball effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This issue has [justifiably] the public's atte...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This issue has [justifiably] the public's atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The occasion provides a broad debate in civil ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The occasion provides a broad debate in civil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The urgent taking of measurements by the guard...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The urgent taking of measurements by the guard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10248</th>\n",
       "      <td>A mayor may belong to the administration of a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A mayor may belong to the administration of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10249</th>\n",
       "      <td>ethically it is reprehensible</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ethically it is reprehensible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10250</th>\n",
       "      <td>ethically is reprehensible and common sense ad...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ethically is reprehensible and common sense ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>common sense advises not to do so</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>common sense advises not to do so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10252</th>\n",
       "      <td>A mayor can have your football club preference</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A mayor can have your football club preference</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10253 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  label article_id  \\\n",
       "0      I assume these shares have generated a snowbal...      0        NaN   \n",
       "1          These shares have generated a snowball effect      0        NaN   \n",
       "2      This issue has [justifiably] the public's atte...      0        NaN   \n",
       "3      The occasion provides a broad debate in civil ...      0        NaN   \n",
       "4      The urgent taking of measurements by the guard...      0        NaN   \n",
       "...                                                  ...    ...        ...   \n",
       "10248  A mayor may belong to the administration of a ...      0        NaN   \n",
       "10249                      ethically it is reprehensible      3        NaN   \n",
       "10250  ethically is reprehensible and common sense ad...      0        NaN   \n",
       "10251                  common sense advises not to do so      0        NaN   \n",
       "10252     A mayor can have your football club preference      0        NaN   \n",
       "\n",
       "                                              translated  \n",
       "0      I assume these shares have generated a snowbal...  \n",
       "1          These shares have generated a snowball effect  \n",
       "2      This issue has [justifiably] the public's atte...  \n",
       "3      The occasion provides a broad debate in civil ...  \n",
       "4      The urgent taking of measurements by the guard...  \n",
       "...                                                  ...  \n",
       "10248  A mayor may belong to the administration of a ...  \n",
       "10249                      ethically it is reprehensible  \n",
       "10250  ethically is reprehensible and common sense ad...  \n",
       "10251                  common sense advises not to do so  \n",
       "10252     A mayor can have your football club preference  \n",
       "\n",
       "[10253 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3386cb",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51d8182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# 90% train, 10% test+validation\n",
    "train_test = dataset_hf.train_test_split(test_size=0.1)\n",
    "\n",
    "# Split the 10% test+validation set in half test, half validation\n",
    "valid_test = train_test['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_valid_test_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb94c69",
   "metadata": {},
   "source": [
    "### Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed39caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1725b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to C:\\Users\\caion\\.cache\\huggingface\\transformers\\tmpxmootr5f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab276fe2f8b45489a91bad802d27d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "creating metadata file for C:\\Users\\caion/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\caion\\.cache\\huggingface\\transformers\\tmps4fjk8en\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a88317cd0d74a9dab2e2cd905f8a554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "creating metadata file for C:\\Users\\caion/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to C:\\Users\\caion\\.cache\\huggingface\\transformers\\tmpg_f2d8fd\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379aa6d1e96848a99620f79bd75c5f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "creating metadata file for C:\\Users\\caion/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to C:\\Users\\caion\\.cache\\huggingface\\transformers\\tmp1d8ry9kx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b4e39261e349679f1703dc98fda780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "creating metadata file for C:\\Users\\caion/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to C:\\Users\\caion\\.cache\\huggingface\\transformers\\tmpzfmfuaeh\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665cf042681d40f481df10924d628c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "creating metadata file for C:\\Users\\caion/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\caion/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, padding=True, truncation=True, model_max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aef4f",
   "metadata": {},
   "source": [
    "## Apply the tokenizer loaded into the text spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bef998b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a2a3bd46934835a64c58955ace08d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d00796bcc94e8a9920603b1e76c748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679a0caf44a645ab9d581121daf4e3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'label', 'article_id'],\n",
       "        num_rows: 9227\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'label', 'article_id'],\n",
       "        num_rows: 513\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'label', 'article_id'],\n",
       "        num_rows: 513\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(sample):\n",
    "    return tokenizer(sample[\"tokens\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = train_valid_test_dataset.map(preprocess_function, batched=True)\n",
    "train_valid_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d6cfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1941, 0.1496, 0.1926, 0.2126, 0.2510]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caion\\AppData\\Local\\Temp/ipykernel_15648/361294440.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = torch.nn.functional.softmax(outputs.logits)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(train_valid_test_dataset['test'][0]['tokens'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bbdcb4",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "The next step is to fine-tune the model to our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ce7c775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results-bert-en-uncased\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\"test_trainer\", num_train_epochs=1)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebb1c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\caion\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9227\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1731\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1731' max='1731' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1731/1731 2:29:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.295000</td>\n",
       "      <td>1.262312</td>\n",
       "      <td>0.532164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.249400</td>\n",
       "      <td>1.237766</td>\n",
       "      <td>0.510721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.185600</td>\n",
       "      <td>1.244944</td>\n",
       "      <td>0.524366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 513\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-bert-en-uncased\\checkpoint-577\n",
      "Configuration saved in ./results-bert-en-uncased\\checkpoint-577\\config.json\n",
      "Model weights saved in ./results-bert-en-uncased\\checkpoint-577\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results-bert-en-uncased\\checkpoint-577\\tokenizer_config.json\n",
      "Special tokens file saved in ./results-bert-en-uncased\\checkpoint-577\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 513\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-bert-en-uncased\\checkpoint-1154\n",
      "Configuration saved in ./results-bert-en-uncased\\checkpoint-1154\\config.json\n",
      "Model weights saved in ./results-bert-en-uncased\\checkpoint-1154\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results-bert-en-uncased\\checkpoint-1154\\tokenizer_config.json\n",
      "Special tokens file saved in ./results-bert-en-uncased\\checkpoint-1154\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 513\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-bert-en-uncased\\checkpoint-1731\n",
      "Configuration saved in ./results-bert-en-uncased\\checkpoint-1731\\config.json\n",
      "Model weights saved in ./results-bert-en-uncased\\checkpoint-1731\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results-bert-en-uncased\\checkpoint-1731\\tokenizer_config.json\n",
      "Special tokens file saved in ./results-bert-en-uncased\\checkpoint-1731\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results-bert-en-uncased\\checkpoint-1154 (score: 1.2377655506134033).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1731, training_loss=1.2275392916903614, metrics={'train_runtime': 8998.9012, 'train_samples_per_second': 3.076, 'train_steps_per_second': 0.192, 'total_flos': 2202864145307880.0, 'train_loss': 1.2275392916903614, 'epoch': 3.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b79270f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: article_id, tokens. If article_id, tokens are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 513\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2377655506134033,\n",
       " 'eval_accuracy': 0.5107212475633528,\n",
       " 'eval_runtime': 43.2918,\n",
       " 'eval_samples_per_second': 11.85,\n",
       " 'eval_steps_per_second': 0.762,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a107fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results-bert-en-uncased\n",
      "Configuration saved in ./results-bert-en-uncased\\config.json\n",
      "Model weights saved in ./results-bert-en-uncased\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results-bert-en-uncased\\tokenizer_config.json\n",
      "Special tokens file saved in ./results-bert-en-uncased\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8ea39",
   "metadata": {},
   "source": [
    "### Loading and using a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1758690f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./results-bert-en-uncased\\added_tokens.json. We won't load it.\n",
      "loading file ./results-bert-en-uncased\\vocab.txt\n",
      "loading file ./results-bert-en-uncased\\tokenizer.json\n",
      "loading file None\n",
      "loading file ./results-bert-en-uncased\\special_tokens_map.json\n",
      "loading file ./results-bert-en-uncased\\tokenizer_config.json\n",
      "loading configuration file ./results-bert-en-uncased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./results-bert-en-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./results-bert-en-uncased\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./results-bert-en-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"./results-bert-en-uncased\")\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"./results-bert-en-uncased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f947be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model2, tokenizer=tokenizer2) #, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b03379d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[219  24   0   5   0]\n",
      " [ 68  28   0   0   0]\n",
      " [ 43   9   0   0   0]\n",
      " [ 86   5   0   3   0]\n",
      " [ 22   1   0   0   0]]\n",
      "Accuracy:  0.4873294346978557\n",
      "Precision:  0.25858208955223877\n",
      "Recall:  0.24132921528254406\n",
      "F1:  0.2081731553269862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caion\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred= []\n",
    "for p in tokenized_dataset['test']['tokens']:\n",
    "    ti = tokenizer2(p, return_tensors=\"pt\")\n",
    "    out = model2(**ti)\n",
    "    pred = torch.argmax(out.logits)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "\n",
    "y_test = tokenized_dataset['test']['label']\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702b207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
